{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7426f7c9-9330-48c2-81a5-b46a24eb087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Pipeline Implementation - Jupyter Notebook\n",
    "# Run each cell sequentially to build and test the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f26a1fb-734f-4ef3-a4aa-86ae91444d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install sentence-transformers scikit-learn beautifulsoup4 requests numpy transformers torch\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install and Import Dependencies\n",
    "\"\"\"\n",
    "!pip install sentence-transformers scikit-learn beautifulsoup4 requests numpy transformers torch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bf16f3-9466-4fb0-92ff-e4f6a1cb2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38e0b15-668f-407c-b918-29bdc8b3c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01ac13a-49a9-4d03-b24e-9bfddd3a359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d39281-4fed-44fe-afd4-612368f7d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74dfa9f5-5c41-4f49-9a01-1469fbdc4bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG System class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 llm_model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG System with embedding model and local LLM\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the sentence transformer model for embeddings\n",
    "            llm_model_name: Name of the local LLM model\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.embedding_model = None\n",
    "        self.llm_pipeline = None\n",
    "        self.tokenizer = None\n",
    "        self.source_url = \"\"\n",
    "        \n",
    "        print(f\"üîß Initializing RAG System...\")\n",
    "        print(f\"üìä Embedding Model: {embedding_model_name}\")\n",
    "        print(f\"üß† LLM Model: {llm_model_name}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load embedding model and local LLM\"\"\"\n",
    "        try:\n",
    "            print(f\"üì• Loading embedding model: {self.embedding_model_name}\")\n",
    "            self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
    "            print(\"‚úÖ Embedding model loaded successfully!\")\n",
    "            \n",
    "            print(f\"üì• Loading local LLM: {self.llm_model_name}\")\n",
    "            # Use a lightweight model for local inference\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            self.llm_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.llm_model_name,\n",
    "                device=device,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=50256\n",
    "            )\n",
    "            print(\"‚úÖ Local LLM loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading models: {str(e)}\")\n",
    "            # Fallback to simpler models\n",
    "            self.embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            print(\"‚ö†Ô∏è Using fallback embedding model\")\n",
    "    \n",
    "    def load_web_content(self, url: str) -> str:\n",
    "        \"\"\"Load and extract text content from a web URL\"\"\"\n",
    "        try:\n",
    "            print(f\"üåê Loading content from: {url}\")\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Extract text\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up text\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            self.source_url = url\n",
    "            print(f\"‚úÖ Successfully loaded {len(text)} characters from URL\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading web content: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def split_text_into_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        print(f\"‚úÇÔ∏è Splitting text into chunks (size: {chunk_size}, overlap: {overlap})\")\n",
    "        \n",
    "        # Clean text\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            \n",
    "            # Try to end at a sentence boundary\n",
    "            if end < len(text):\n",
    "                sentence_end = max(\n",
    "                    text.rfind('.', start, end),\n",
    "                    text.rfind('!', start, end),\n",
    "                    text.rfind('?', start, end)\n",
    "                )\n",
    "                \n",
    "                if sentence_end > start + chunk_size // 2:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            start = max(start + chunk_size - overlap, end)\n",
    "        \n",
    "        self.chunks = chunks\n",
    "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def create_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for all chunks\"\"\"\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No chunks available. Please load and split text first.\")\n",
    "        \n",
    "        print(\"üî¢ Creating embeddings for chunks...\")\n",
    "        self.embeddings = self.embedding_model.encode(self.chunks, show_progress_bar=True)\n",
    "        print(f\"‚úÖ Created embeddings with shape: {self.embeddings.shape}\")\n",
    "        return self.embeddings\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant chunks using cosine similarity\n",
    "        \n",
    "        Method Explanation:\n",
    "        1. Convert query to embedding using same model as chunks\n",
    "        2. Calculate cosine similarity between query and all chunk embeddings\n",
    "        3. Cosine similarity measures angle between vectors (0-1, higher = more similar)\n",
    "        4. Select top-k chunks with highest similarity scores\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not created. Please create embeddings first.\")\n",
    "        \n",
    "        print(f\"üîç Retrieving relevant chunks for query: '{query}'\")\n",
    "        print(f\"üìä Using cosine similarity for vector search\")\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar chunks\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            relevant_chunks.append((\n",
    "                self.chunks[idx],\n",
    "                float(similarities[idx]),\n",
    "                int(idx)\n",
    "            ))\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(relevant_chunks)} relevant chunks\")\n",
    "        print(f\"üìà Similarity scores: {[f'{score:.3f}' for _, score, _ in relevant_chunks]}\")\n",
    "        return relevant_chunks\n",
    "    \n",
    "    def extract_answer_from_chunks(self, query: str, relevant_chunks: List[Tuple[str, float, int]], \n",
    "                                 similarity_threshold: float = 0.2) -> Dict:\n",
    "        \"\"\"Extract answer from relevant chunks using content analysis\"\"\"\n",
    "        print(\"üß† Extracting answer from chunks using content analysis...\")\n",
    "        \n",
    "        # Filter chunks by similarity threshold\n",
    "        filtered_chunks = [(chunk, score, idx) for chunk, score, idx in relevant_chunks \n",
    "                          if score >= similarity_threshold]\n",
    "        \n",
    "        if not filtered_chunks:\n",
    "            return {\n",
    "                \"answer\": \"‚ùå I couldn't find relevant information to answer your question in the provided content. The similarity scores were too low, suggesting the content may not contain information related to your query.\",\n",
    "                \"chunks_used\": [],\n",
    "                \"method\": \"Content Analysis - No relevant chunks found above similarity threshold\",\n",
    "                \"similarity_scores\": [],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Content analysis approach\n",
    "        query_words = set(query.lower().split())\n",
    "        query_words = {word for word in query_words if len(word) > 2}  # Filter short words\n",
    "        \n",
    "        best_chunk = filtered_chunks[0]  # Highest similarity chunk\n",
    "        answer_text = best_chunk[0]\n",
    "        \n",
    "        # Extract sentences that contain query keywords\n",
    "        sentences = re.split(r'[.!?]+', answer_text)\n",
    "        relevant_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                sentence_words = set(sentence.lower().split())\n",
    "                # Check if sentence contains query keywords\n",
    "                if query_words.intersection(sentence_words):\n",
    "                    relevant_sentences.append(sentence)\n",
    "        \n",
    "        if relevant_sentences:\n",
    "            answer = '. '.join(relevant_sentences[:2])  # Take first 2 relevant sentences\n",
    "            confidence = filtered_chunks[0][1]  # Use highest similarity as confidence\n",
    "        else:\n",
    "            # Fallback to first part of best chunk\n",
    "            answer = answer_text[:400] + \"...\" if len(answer_text) > 400 else answer_text\n",
    "            confidence = filtered_chunks[0][1] * 0.8  # Lower confidence for fallback\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"chunks_used\": [{\"chunk_index\": idx, \"chunk_text\": chunk, \"similarity_score\": score}\n",
    "                           for chunk, score, idx in filtered_chunks],\n",
    "            \"method\": \"Content Analysis - Keyword matching and sentence extraction from most similar chunks\",\n",
    "            \"similarity_scores\": [score for _, score, _ in filtered_chunks],\n",
    "            \"confidence\": confidence,\n",
    "            \"source_url\": self.source_url\n",
    "        }\n",
    "    \n",
    "    def generate_llm_answer(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using local LLM\"\"\"\n",
    "        try:\n",
    "            print(\"üß† Generating answer using local LLM...\")\n",
    "            prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "            \n",
    "            if self.llm_pipeline:\n",
    "                response = self.llm_pipeline(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "                generated_text = response[0]['generated_text']\n",
    "                \n",
    "                # Extract only the answer part\n",
    "                answer_start = generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
    "                answer = generated_text[answer_start:].strip()\n",
    "                \n",
    "                return answer if answer else \"Unable to generate answer with local LLM.\"\n",
    "            else:\n",
    "                return \"Local LLM not available.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating LLM answer: {str(e)}\")\n",
    "            return \"Error occurred while generating answer with local LLM.\"\n",
    "    \n",
    "    def query(self, question: str, use_llm: bool = False, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Main query method that orchestrates the RAG pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç PROCESSING QUERY: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Retrieve relevant chunks\n",
    "            relevant_chunks = self.retrieve_relevant_chunks(question, top_k=top_k)\n",
    "            \n",
    "            if not use_llm:\n",
    "                # Extract answer without LLM\n",
    "                result = self.extract_answer_from_chunks(question, relevant_chunks)\n",
    "                result[\"llm_used\"] = False\n",
    "            else:\n",
    "                # Use local LLM for answer generation\n",
    "                context = \" \".join([chunk for chunk, _, _ in relevant_chunks[:2]])\n",
    "                llm_answer = self.generate_llm_answer(question, context)\n",
    "                \n",
    "                result = {\n",
    "                    \"answer\": llm_answer,\n",
    "                    \"chunks_used\": [{\"chunk_index\": idx, \"chunk_text\": chunk, \"similarity_score\": score}\n",
    "                                   for chunk, score, idx in relevant_chunks],\n",
    "                    \"method\": \"Local LLM generation with retrieved context\",\n",
    "                    \"similarity_scores\": [score for _, score, _ in relevant_chunks],\n",
    "                    \"confidence\": max([score for _, score, _ in relevant_chunks]) if relevant_chunks else 0.0,\n",
    "                    \"llm_used\": True,\n",
    "                    \"source_url\": self.source_url\n",
    "                }\n",
    "            \n",
    "            result[\"query\"] = question\n",
    "            result[\"processing_time\"] = time.time() - start_time\n",
    "            \n",
    "            print(f\"‚úÖ Query processed in {result['processing_time']:.2f} seconds\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing query: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": f\"Error processing query: {str(e)}\",\n",
    "                \"chunks_used\": [],\n",
    "                \"method\": \"Error occurred\",\n",
    "                \"similarity_scores\": [],\n",
    "                \"query\": question,\n",
    "                \"confidence\": 0.0,\n",
    "                \"llm_used\": use_llm,\n",
    "                \"processing_time\": 0.0\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ RAG System class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60895f29-097f-4030-beb1-8c99c35dacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbfe752-ed8e-4714-8b68-7c8961a2c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG System...\n",
      "üîß Initializing RAG System...\n",
      "üìä Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üß† LLM Model: microsoft/DialoGPT-medium\n",
      "üì• Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Embedding model loaded successfully!\n",
      "üì• Loading local LLM: microsoft/DialoGPT-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local LLM loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Initializing RAG System...\")\n",
    "rag = RAGSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bc985a-78e2-4183-8cca-fe40d99cd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Process Content\n",
    "# # Choose one of the sample URLs or provide your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa13dc12-8247-4c0b-8089-9e4cf44618f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls = [\n",
    "    \"https://genai.owasp.org/initiatives/\",\n",
    "    \"https://www.stanford.edu/research/\",\n",
    "    \"https://openai.com/about/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b83108-471b-4a60-8c60-ddcba0f643b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Selected URL: https://genai.owasp.org/initiatives/\n"
     ]
    }
   ],
   "source": [
    "# Select URL (change index to try different URLs)\n",
    "selected_url = sample_urls[0]  # Change to 1 or 2 for other URLs\n",
    "print(f\"üåê Selected URL: {selected_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c28f7c15-8a08-40f7-a822-025c50057524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Loading content from: https://genai.owasp.org/initiatives/\n",
      "‚úÖ Successfully loaded 6267 characters from URL\n",
      "üìÑ Content preview (first 300 chars):\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 2023/24 AI Security Landscape GOVERNANCE CHECKLIST Threat Intelligence AGENTIC APP SECURITY Secure AI A...\n",
      "‚úÇÔ∏è Splitting text into chunks (size: 500, overlap: 50)\n",
      "‚úÖ Created 14 chunks\n",
      "\n",
      "üìù Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES...\n",
      "\n",
      "Chunk 2:\n",
      "NDUSTRY RECOGNITION Governance CONTACT BRANDING T10 FOR GEN AI Project Initiatives Initiatives The goal of initiatives within the project are to addre...\n",
      "\n",
      "Chunk 3:\n",
      "lligence Secure Gen AI Adoption Risk and Data Gathering AI Red Teaming and Eval Agentic Application Security AI Cyber Threat Intelligence Limited acti...\n",
      "üî¢ Creating embeddings for chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caedf36e3994d6394aaec3eebf608cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created embeddings with shape: (14, 384)\n",
      "\n",
      "üî¢ Embeddings created successfully!\n",
      "Shape: (14, 384)\n",
      "Sample embedding (first 10 values): [-0.00336468 -0.04504144  0.01070712  0.0016239   0.06725973  0.02016295\n",
      "  0.05085123 -0.0131855  -0.03454284  0.01050522]\n"
     ]
    }
   ],
   "source": [
    "# Load content\n",
    "try:\n",
    "    content = rag.load_web_content(selected_url)\n",
    "    print(f\"üìÑ Content preview (first 300 chars):\\n{content[:300]}...\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = rag.split_text_into_chunks(content, chunk_size=500, overlap=50)\n",
    "    print(f\"\\nüìù Sample chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"{chunk[:150]}...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = rag.create_embeddings()\n",
    "    print(f\"\\nüî¢ Embeddings created successfully!\")\n",
    "    print(f\"Shape: {embeddings.shape}\")\n",
    "    print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47213fa1-ced5-43e7-9935-db92a40bbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Queries - Without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fb4fa5-0a2a-4486-ae36-56c07d3be16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß™ TESTING QUERIES WITHOUT LLM\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Query 1: What are the main initiatives mentioned?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: What are the main initiatives mentioned?\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'What are the main initiatives mentioned?'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e62d41ee8674b7f87772b17cd497088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.564', '0.413', '0.409']\n",
      "üß† Extracting answer from chunks using content analysis...\n",
      "‚úÖ Query processed in 0.17 seconds\n",
      "ü§ñ ANSWER:\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 2023/24 AI Security Landscape GOVERNANCE CHECKLIST Threat Intelligence AGENTIC APP SECURITY Secure AI Adoption AI Red Teaming Data Security PROJECT Mission and Charter ROADMAP LEADERSHIP CONTRIBUTORS SPONSORS SUPPORTERS SPONSORSHIP NEWSLETTER OWASP PROJECT PAGE PROJECT WIKI BLOG ABOUT EVENTS NEWSROOM I\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Content Analysis - Keyword matching and sentence extraction from most similar chunks\n",
      "Confidence: 0.564\n",
      "Processing Time: 0.17s\n",
      "Chunks Used: 3\n",
      "\n",
      "üìÑ CHUNKS USED:\n",
      "Chunk 1 (Index: 0, Similarity: 0.564):\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 202...\n",
      "\n",
      "Chunk 2 (Index: 1, Similarity: 0.413):\n",
      "NDUSTRY RECOGNITION Governance CONTACT BRANDING T10 FOR GEN AI Project Initiatives Initiatives The goal of initiatives within the project are to address specific areas, of education and research to cr...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Query 2: What topics are covered in this content?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: What topics are covered in this content?\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'What topics are covered in this content?'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d19eac9afd48c6b5bdc050f3638cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.333', '0.266', '0.266']\n",
      "üß† Extracting answer from chunks using content analysis...\n",
      "‚úÖ Query processed in 0.06 seconds\n",
      "ü§ñ ANSWER:\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 2023/24 AI Security Landscape GOVERNANCE CHECKLIST Threat Intelligence AGENTIC APP SECURITY Secure AI Adoption AI Red Teaming Data Security PROJECT Mission and Charter ROADMAP LEADERSHIP CONTRIBUTORS SPO...\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Content Analysis - Keyword matching and sentence extraction from most similar chunks\n",
      "Confidence: 0.267\n",
      "Processing Time: 0.06s\n",
      "Chunks Used: 3\n",
      "\n",
      "üìÑ CHUNKS USED:\n",
      "Chunk 1 (Index: 0, Similarity: 0.333):\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 202...\n",
      "\n",
      "Chunk 2 (Index: 1, Similarity: 0.266):\n",
      "NDUSTRY RECOGNITION Governance CONTACT BRANDING T10 FOR GEN AI Project Initiatives Initiatives The goal of initiatives within the project are to address specific areas, of education and research to cr...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Query 3: What are the key focus areas?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: What are the key focus areas?\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'What are the key focus areas?'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cad081500640cead51b97fcd6feb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.274', '0.215', '0.202']\n",
      "üß† Extracting answer from chunks using content analysis...\n",
      "‚úÖ Query processed in 0.04 seconds\n",
      "ü§ñ ANSWER:\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 2023/24 AI Security Landscape GOVERNANCE CHECKLIST Threat Intelligence AGENTIC APP SECURITY Secure AI Adoption AI Red Teaming Data Security PROJECT Mission and Charter ROADMAP LEADERSHIP CONTRIBUTORS SPO...\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Content Analysis - Keyword matching and sentence extraction from most similar chunks\n",
      "Confidence: 0.219\n",
      "Processing Time: 0.04s\n",
      "Chunks Used: 3\n",
      "\n",
      "üìÑ CHUNKS USED:\n",
      "Chunk 1 (Index: 0, Similarity: 0.274):\n",
      "Initiatives - OWASP Gen AI Security Project Skip to content GETTING STARTED Introduction LEARNING MEETINGS RESOURCES CONTRIBUTING GLOSSARY INITIATIVES LLM TOP 10 LLM TOP 10 FOR 2025 LLM TOP 10 FOR 202...\n",
      "\n",
      "Chunk 2 (Index: 7, Similarity: 0.215):\n",
      "hodology, the initiative seeks to enhance AI security guidelines and provide valuable insights for organizations to strengthen their LLM-based systems. Guidance & Resources Data Gathering Methology wi...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Query 4: Tell me about the research mentioned\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: Tell me about the research mentioned\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'Tell me about the research mentioned'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8dcff36fb740efbc1b321648abd10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.270', '0.234', '0.218']\n",
      "üß† Extracting answer from chunks using content analysis...\n",
      "‚úÖ Query processed in 0.05 seconds\n",
      "ü§ñ ANSWER:\n",
      "Research Initiative: AI Red Teaming & Evaluation Red Teaming: The Power of Adversarial Thinking in AI Security (AI hackers, tech wizards, and code sorcerers, we need you. Research Initiative ‚Äì Securing and Scrutinizing LLMS in Exploit Generation Challenge Currently limited actionable data exists in understanding how different LLMS are being leveraged in exploit generation, and what mechanisms can be used to\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Content Analysis - Keyword matching and sentence extraction from most similar chunks\n",
      "Confidence: 0.270\n",
      "Processing Time: 0.05s\n",
      "Chunks Used: 3\n",
      "\n",
      "üìÑ CHUNKS USED:\n",
      "Chunk 1 (Index: 11, Similarity: 0.270):\n",
      ". Research Initiative: AI Red Teaming & Evaluation Red Teaming: The Power of Adversarial Thinking in AI Security (AI hackers, tech wizards, and code sorcerers, we need you!) This is your invitation......\n",
      "\n",
      "Chunk 2 (Index: 13, Similarity: 0.234):\n",
      "‚Äì Copyright ¬© 2025 OWASP Foundation, Inc. Scroll to Top Loading Comments... Write a Comment... Email (Required) Name (Required) Website...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ TESTING QUERIES WITHOUT LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the main initiatives mentioned?\",\n",
    "    \"What topics are covered in this content?\",\n",
    "    \"What are the key focus areas?\",\n",
    "    \"Tell me about the research mentioned\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = rag.query(query, use_llm=False, top_k=3)\n",
    "    \n",
    "    print(f\"ü§ñ ANSWER:\")\n",
    "    print(f\"{result['answer']}\")\n",
    "    print(f\"\\nüìä METADATA:\")\n",
    "    print(f\"Method: {result['method']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
    "    print(f\"Chunks Used: {len(result['chunks_used'])}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ CHUNKS USED:\")\n",
    "    for j, chunk_info in enumerate(result['chunks_used'][:2]):  # Show first 2 chunks\n",
    "        print(f\"Chunk {j+1} (Index: {chunk_info['chunk_index']}, Similarity: {chunk_info['similarity_score']:.3f}):\")\n",
    "        print(f\"{chunk_info['chunk_text'][:200]}...\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bda7f99-e12d-446c-8e42-5b9373bc77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Queries - With LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa27252-c64b-4e2e-aa5a-faeb3018eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß† TESTING QUERIES WITH LOCAL LLM\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "LLM Query 1: What are the main initiatives mentioned?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: What are the main initiatives mentioned?\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'What are the main initiatives mentioned?'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3147bc85cf584eed817b159abd6ed355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.564', '0.413', '0.409']\n",
      "üß† Generating answer using local LLM...\n",
      "‚úÖ Query processed in 0.55 seconds\n",
      "ü§ñ LLM ANSWER:\n",
      "Unable to generate answer with local LLM.\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Local LLM generation with retrieved context\n",
      "Confidence: 0.564\n",
      "Processing Time: 0.55s\n",
      "Chunks Used: 3\n",
      "\n",
      "============================================================\n",
      "LLM Query 2: What topics are covered in this content?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING QUERY: What topics are covered in this content?\n",
      "============================================================\n",
      "üîç Retrieving relevant chunks for query: 'What topics are covered in this content?'\n",
      "üìä Using cosine similarity for vector search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b0ab7307646808b037e826ed66ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìà Similarity scores: ['0.333', '0.266', '0.266']\n",
      "üß† Generating answer using local LLM...\n",
      "‚úÖ Query processed in 0.12 seconds\n",
      "ü§ñ LLM ANSWER:\n",
      "Unable to generate answer with local LLM.\n",
      "\n",
      "üìä METADATA:\n",
      "Method: Local LLM generation with retrieved context\n",
      "Confidence: 0.333\n",
      "Processing Time: 0.12s\n",
      "Chunks Used: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† TESTING QUERIES WITH LOCAL LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(test_queries[:2], 1):  # Test first 2 queries with LLM\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LLM Query {i}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = rag.query(query, use_llm=True, top_k=3)\n",
    "    \n",
    "    print(f\"ü§ñ LLM ANSWER:\")\n",
    "    print(f\"{result['answer']}\")\n",
    "    print(f\"\\nüìä METADATA:\")\n",
    "    print(f\"Method: {result['method']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
    "    print(f\"Chunks Used: {len(result['chunks_used'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8da8ac7-39fb-4cd9-b358-b16768f55077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea5654e-bda8-4e77-95a4-a8dc025bf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_query():\n",
    "    \"\"\"Interactive interface for testing queries\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ INTERACTIVE QUERY INTERFACE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Enter your questions below (type 'quit' to exit)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\n‚ùì Your question: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_query:\n",
    "                continue\n",
    "                \n",
    "            # Ask for LLM preference\n",
    "            use_llm_input = input(\"üß† Use local LLM? (y/n, default=n): \").strip().lower()\n",
    "            use_llm = use_llm_input in ['y', 'yes', '1', 'true']\n",
    "            \n",
    "            # Process query\n",
    "            result = rag.query(user_query, use_llm=use_llm, top_k=3)\n",
    "            \n",
    "            print(f\"\\nü§ñ ANSWER:\")\n",
    "            print(f\"{result['answer']}\")\n",
    "            \n",
    "            print(f\"\\nüìä DETAILS:\")\n",
    "            print(f\"Method: {result['method']}\")\n",
    "            print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"LLM Used: {'Yes' if result['llm_used'] else 'No'}\")\n",
    "            print(f\"Chunks Used: {len(result['chunks_used'])}\")\n",
    "            \n",
    "            # Show chunk sources\n",
    "            if result['chunks_used']:\n",
    "                print(f\"\\nüìÑ SOURCE CHUNKS:\")\n",
    "                for i, chunk_info in enumerate(result['chunks_used'][:2]):\n",
    "                    print(f\"  {i+1}. Similarity: {chunk_info['similarity_score']:.3f}\")\n",
    "                    print(f\"     Preview: {chunk_info['chunk_text'][:100]}...\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c875d8-a893-4ddb-ac6e-8effb0cf2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    " interactive_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eb525-3857-4549-86e4-035a5d85d192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_system",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
